
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Embeddings &#8212; DL Notes 1.0 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neural Embeddings" href="neural_embeddings.html" />
    <link rel="prev" title="NLP" href="index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="embeddings">
<h1>Embeddings<a class="headerlink" href="#embeddings" title="Permalink to this headline">¶</a></h1>
<div class="section" id="what">
<h2>What<a class="headerlink" href="#what" title="Permalink to this headline">¶</a></h2>
<p>Embeddings are ways of representing words or documents of words as numbers (technically vectors of numbers).</p>
</div>
<div class="section" id="why">
<h2>Why<a class="headerlink" href="#why" title="Permalink to this headline">¶</a></h2>
<p>Basically, computers work with numbers, but not words</p>
</div>
<div class="section" id="how">
<h2>How<a class="headerlink" href="#how" title="Permalink to this headline">¶</a></h2>
<p>There are a bunch of ways</p>
</div>
<div class="section" id="term-document-matrix">
<h2>Term Document Matrix<a class="headerlink" href="#term-document-matrix" title="Permalink to this headline">¶</a></h2>
<p>We can use a term document matrix to create embeddings for documents OR words as follows:</p>
<p>Given a list of documents (Tweets) and counted occurences of words in them,
we can simply represent each document by its corresponding column of words.</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Data</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"></th>
<th class="head"><p>Tweet 1</p></th>
<th class="head"><p>Tweet 2</p></th>
<th class="head"><p>Tweet 3</p></th>
<th class="head"><p>Tweet 4</p></th>
<th class="head"><p>More Tweets…</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>the</p></th>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>7</p></td>
<td><p>4</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>car</p></th>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>walk</p></th>
<td><p>3</p></td>
<td><p>1</p></td>
<td><p>4</p></td>
<td><p>5</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>More words…</p></th>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
</tbody>
</table>
<p>Thus, Tweet 1 would be represented as the vector [2, 0, 3, …]
We could also represent a word by its corresponding row of documents, so
“the” could be represented as [2, 3, 7, 4, …]</p>
<p>Problems:</p>
<ul class="simple">
<li><p>Common words like “the” will have very large counts and depending on the distance metrics,
this could make ALL document embeddings look the same since “the” will contribute much more to
the metric than other words.</p></li>
</ul>
</div>
<div class="section" id="word-word-matrix">
<h2>Word-Word Matrix<a class="headerlink" href="#word-word-matrix" title="Permalink to this headline">¶</a></h2>
<p>Comment 1: “I love sweaters”</p>
<p>Comment 2: “I like sweaters”</p>
<p>Comment 3: “I like frogs”</p>
<table class="docutils align-default" id="id2">
<caption><span class="caption-text">Data</span><a class="headerlink" href="#id2" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"></th>
<th class="head"><p>I</p></th>
<th class="head"><p>love</p></th>
<th class="head"><p>sweaters</p></th>
<th class="head"><p>like</p></th>
<th class="head"><p>frogs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>I</p></th>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>love</p></th>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>sweaters</p></th>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>like</p></th>
<td><p>2</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>frogs</p></th>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Now, we can represent the word “love” as the vector [0, 1, 0, 2, 0]</p>
<p>Problems:</p>
<ul class="simple">
<li><p>Common words like “the” will have very large counts and depending on the distance metrics,
this could make ALL document embeddings look the same since “the” will contribute much more to
the metric than other words.</p></li>
</ul>
</div>
<div class="section" id="tf-idf">
<h2>TF-IDF<a class="headerlink" href="#tf-idf" title="Permalink to this headline">¶</a></h2>
<p>TF-IDF works towards reducing how much very common words like “the” contribute to word embeddings.
TD-IDF has two parts,</p>
<ol class="arabic simple">
<li><p>TF</p></li>
</ol>
<p>TF is the same as the vector we made with the term document matrix.
It is just a count of the words occurences across documents.</p>
<ol class="arabic simple" start="2">
<li><p>IDF</p></li>
</ol>
<p>IDF is a scalar computed per word as</p>
<div class="math notranslate nohighlight">
\[log (\frac{ \text{# documents} }{\text{# documents in which word appears}})\]</div>
<p>It scores words which occur in few documents higher than words which occur in many documents.
This makes very common words less important.</p>
<p>TF-IDF then is just TF * IDF</p>
</div>
<div class="section" id="pointwise-mutual-information">
<h2>Pointwise Mutual Information<a class="headerlink" href="#pointwise-mutual-information" title="Permalink to this headline">¶</a></h2>
<p>PMI is computed on word-word matrices and asks whether words are more or less likely
to co-occur within a context window.</p>
<p>Its formula is:</p>
<div class="math notranslate nohighlight">
\[PMI(X, Y) = log_2 (\frac{P(X, Y)}{P(X) * P(Y)})\]</div>
<p>Where X and Y are two words.</p>
<p>P(X, Y) is estimated as the number of times X and Y occur in the same context window
divided by the sum of all counts in the table.</p>
<p>P(X) and P(Y) are each estimated as the number of times the word occurs
divided by the table sum.</p>
<p>If PMI &gt; 0, this indicates that if we have X or Y, the other word will be more
likely to occur in the context window.</p>
<p>If PMI &lt; 0, this theoretically indicates that if we know one word occurs,
the other will be less likely to occur.</p>
<p>However:</p>
<p>Problems:</p>
<ul class="simple">
<li><p>Negative values unreliable</p></li>
</ul>
</div>
<div class="section" id="ppmi">
<h2>PPMI<a class="headerlink" href="#ppmi" title="Permalink to this headline">¶</a></h2>
<p>Just set &lt; 0 values to 0</p>
<p>Problems:</p>
<ul class="simple">
<li><p>Biased towards infrequent events; infrequent events have very high PMIs</p></li>
</ul>
</div>
<div class="section" id="weighted-ppmi">
<h2>Weighted PPMI<a class="headerlink" href="#weighted-ppmi" title="Permalink to this headline">¶</a></h2>
<p>Modify probability of each word as:</p>
<div class="math notranslate nohighlight">
\[P_a(i) = \frac{count(i)^a}{\text{# total words}^a}\]</div>
<p>Effect:
* Smooths probability, so less super high vals</p>
</div>
<div class="section" id="add-k-ppmi">
<h2>Add k PPMI<a class="headerlink" href="#add-k-ppmi" title="Permalink to this headline">¶</a></h2>
<p>Just add k to all counts</p>
<p>Effect:
* Smooths probability, so less super high vals</p>
</div>
<div class="section" id="excercises">
<h2>Excercises<a class="headerlink" href="#excercises" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Derive TF-IDF from scratch.</p></li>
<li><p>Compute the TF-IDF of a word in the first grid</p></li>
<li><p>Derive PPMI from scratch</p></li>
<li><p>Compute the PPMI of a word in the second grid</p></li>
</ul>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">DL Notes</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../ML/index.html">ML</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">NLP</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Embeddings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#what">What</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why">Why</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how">How</a></li>
<li class="toctree-l3"><a class="reference internal" href="#term-document-matrix">Term Document Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#word-word-matrix">Word-Word Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tf-idf">TF-IDF</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pointwise-mutual-information">Pointwise Mutual Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ppmi">PPMI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#weighted-ppmi">Weighted PPMI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#add-k-ppmi">Add k PPMI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#excercises">Excercises</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neural_embeddings.html">Neural Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="similarity.html">Similarity Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="meta.html">Meta</a></li>
<li class="toctree-l2"><a class="reference internal" href="n-grams.html">N Grams</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluating_LMs.html">Evaluating LMs</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">NLP</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">NLP</a></li>
      <li>Next: <a href="neural_embeddings.html" title="next chapter">Neural Embeddings</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;None.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/NLP/embeddings.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>