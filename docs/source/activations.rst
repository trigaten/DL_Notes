
=============================================================
Activations
=============================================================

-------
ReLu
-------

faster training - AlexNet

ReLU activation function makes model training easier when using different parameter initialization methods - D2L